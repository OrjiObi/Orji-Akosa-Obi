{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3639953,"sourceType":"datasetVersion","datasetId":2180053}],"dockerImageVersionId":30474,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Task 1: Working with Bag of Words Dataset**","metadata":{}},{"cell_type":"markdown","source":"Importing important and necessary libraries ","metadata":{}},{"cell_type":"code","source":"#Importing important and necessary libraries\n\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:32:09.049563Z","iopub.execute_input":"2024-07-08T11:32:09.049942Z","iopub.status.idle":"2024-07-08T11:32:10.649757Z","shell.execute_reply.started":"2024-07-08T11:32:09.049913Z","shell.execute_reply":"2024-07-08T11:32:10.648613Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The UCI Bag of Words dataset is a collection of text documents that have been preprocessed and represented as a bag-of-words model. It is commonly used in natural language processing and machine learning tasks, such as text classification and information retrieval.\n\nThe dataset consists of a text corpus where each document is represented as a sparse vector of word frequencies. The bag-of-words model represents a document by counting the frequency of each word that appears in it, ignoring the order and structure of the text. This representation allows for efficient and straightforward analysis of text data.\n\nThe UCI Bag of Words dataset includes two files:\n\n* vocab.txt: This file contains the vocabulary of the dataset, listing all the unique words found in the corpus. Each word is assigned a unique identifier or index.\n\n* docword.txt: This file represents the bag-of-words representation of the documents. It contains three columns: docID, wordID, and count. Each row corresponds to an occurrence of a word in a document. The docID identifies the document, the wordID represents the index of the word in the vocabulary, and the count indicates the frequency of that word in the document.","metadata":{}},{"cell_type":"markdown","source":"I combined three datasets into a common corpus. Therefore, two functions called **get_bow_file** and **get_vocab_file** are created to write these datasets into pandas dataframe one-by-one.","metadata":{}},{"cell_type":"code","source":"# Define paths to datasets\ndata_paths = {\n    'enron': {\n        'docword': '../input/uci-bag-of-words/docword.enron.txt',\n        'vocab': '../input/uci-bag-of-words/vocab.enron.txt'\n    },\n    'kos': {\n        'docword': '../input/uci-bag-of-words/docword.kos.txt',\n        'vocab': '../input/uci-bag-of-words/vocab.kos.txt'\n    },\n    'nips': {\n        'docword': '../input/uci-bag-of-words/docword.nips.txt',\n        'vocab': '../input/uci-bag-of-words/vocab.nips.txt'\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:55:02.618274Z","iopub.execute_input":"2024-07-08T11:55:02.619367Z","iopub.status.idle":"2024-07-08T11:55:02.626812Z","shell.execute_reply.started":"2024-07-08T11:55:02.619323Z","shell.execute_reply":"2024-07-08T11:55:02.625336Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Load dataset into pandas DataFrame\ndef load_dataset(data_paths, dataset_name, doc_limit=None):\n    docword_path = data_paths[dataset_name]['docword']\n    vocab_path = data_paths[dataset_name]['vocab']\n    \n    docword = pd.read_csv(docword_path, header=None, names=['docID', 'wordID', 'count'], skiprows=3, sep=' ')\n    vocab = pd.read_csv(vocab_path, header=None, names=['word']).fillna('null')\n    vocab['wordID'] = vocab.index + 1\n    \n    if doc_limit:\n        doc_ids = random.sample(list(set(docword['docID'])), k=doc_limit)\n        docword = docword[docword['docID'].isin(doc_ids)].reset_index(drop=True)\n    \n    return docword, vocab","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:55:38.552727Z","iopub.execute_input":"2024-07-08T11:55:38.553130Z","iopub.status.idle":"2024-07-08T11:55:38.561209Z","shell.execute_reply.started":"2024-07-08T11:55:38.553097Z","shell.execute_reply":"2024-07-08T11:55:38.559967Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Load datasets\nenron, enron_vocab = load_dataset(data_paths, 'enron', doc_limit=6000)\nkos, kos_vocab = load_dataset(data_paths, 'kos')\nnips, nips_vocab = load_dataset(data_paths, 'nips')","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:55:58.943723Z","iopub.execute_input":"2024-07-08T11:55:58.944099Z","iopub.status.idle":"2024-07-08T11:56:00.555485Z","shell.execute_reply.started":"2024-07-08T11:55:58.944072Z","shell.execute_reply":"2024-07-08T11:56:00.553467Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Combine datasets into a common corpus\ndfs = []\noffset = 0\nfor df, vocab in [(enron, enron_vocab), (kos, kos_vocab), (nips, nips_vocab)]:\n    ids = df['docID'] + offset\n    df['new_id'] = ids\n    offset = ids.max()\n    df = df.merge(vocab)[['new_id', 'word', 'count']]\n    dfs.append(df)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:56:32.604738Z","iopub.execute_input":"2024-07-08T11:56:32.605155Z","iopub.status.idle":"2024-07-08T11:56:32.908445Z","shell.execute_reply.started":"2024-07-08T11:56:32.605120Z","shell.execute_reply":"2024-07-08T11:56:32.907329Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"merged = pd.concat(dfs, ignore_index=True).rename(columns={'new_id': 'docID'})\nmerged_vocab = pd.DataFrame({'word': merged['word'].unique()}).reset_index().rename(columns={'index': 'wordID'})\nmerged = merged.merge(merged_vocab, how='left')\nmerged = merged[['docID', 'wordID', 'count']].sort_values(['docID', 'wordID']).reset_index(drop=True)\n\n# Create word-document matrix\nwdm = merged.pivot(index='wordID', columns='docID', values='count').fillna(0.0).astype(pd.SparseDtype(\"float\", 0.0))","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:57:02.152652Z","iopub.execute_input":"2024-07-08T11:57:02.153741Z","iopub.status.idle":"2024-07-08T11:57:14.982806Z","shell.execute_reply.started":"2024-07-08T11:57:02.153696Z","shell.execute_reply":"2024-07-08T11:57:14.981685Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Truncated SVD\nsvd = TruncatedSVD(n_components=100, n_iter=10, random_state=42)\ny_svd = svd.fit_transform(wdm)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:58:35.222044Z","iopub.execute_input":"2024-07-08T11:58:35.222765Z","iopub.status.idle":"2024-07-08T11:58:41.301540Z","shell.execute_reply.started":"2024-07-08T11:58:35.222706Z","shell.execute_reply":"2024-07-08T11:58:41.300554Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# Top dimensions analysis\nwordsSVD = pd.DataFrame(y_svd, index=merged_vocab['word'])\n\nfor index in range(10):\n    thrsh = wordsSVD.loc[:, index].quantile(0.9)\n    print(f\"Dimension: {index}\")\n    words = list(wordsSVD[wordsSVD[index] > thrsh].sort_values([index], ascending=False).index)[:25]\n    print(words)\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:58:45.350194Z","iopub.execute_input":"2024-07-08T11:58:45.351153Z","iopub.status.idle":"2024-07-08T11:58:45.413486Z","shell.execute_reply.started":"2024-07-08T11:58:45.351112Z","shell.execute_reply":"2024-07-08T11:58:45.412438Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Dimension: 0\n['network', 'model', 'learning', 'input', 'function', 'neural', 'set', 'unit', 'data', 'training', 'algorithm', 'system', 'output', 'weight', 'error', 'problem', 'result', 'number', 'method', 'parameter', 'pattern', 'neuron', 'vector', 'point', 'layer']\n\nDimension: 1\n['company', 'power', 'energy', 'california', 'electricity', 'market', 'billion', 'davis', 'business', 'plan', 'companies', 'firm', 'stock', 'prices', 'price', 'customer', 'plant', 'cost', 'states', 'bill', 'financial', 'month', 'utility', 'investor', 'group']\n\nDimension: 2\n['network', 'unit', 'input', 'neural', 'output', 'weight', 'layer', 'hidden', 'net', 'training', 'company', 'pattern', 'connection', 'recurrent', 'neuron', 'architecture', 'activation', 'trained', 'firm', 'propagation', 'chip', 'delay', 'fund', 'analog', 'threshold']\n\nDimension: 3\n['model', 'neuron', 'cell', 'input', 'visual', 'system', 'response', 'signal', 'object', 'field', 'motion', 'activity', 'firing', 'direction', 'synaptic', 'stimulus', 'cortex', 'frequency', 'neural', 'circuit', 'spike', 'connection', 'cortical', 'orientation', 'spatial']\n\nDimension: 4\n['pst', 'columbia', 'mid', 'avista', 'aquila', 'epme', 'morgan', 'bpa', 'idacorpene', 'mieco', 'detm', 'emmt', 'aet', 'hafslund', 'eesi', 'engage', 'montana', 'sncl', 'psc', 'border', 'pinwest', 'chelan', 'sink', 'cinergy', 'pget']\n\nDimension: 5\n['company', 'firm', 'fund', 'investor', 'partner', 'management', 'ventures', 'technology', 'capital', 'round', 'investment', 'product', 'venturewire', 'services', 'software', 'data', 'top', 'mail', 'financial', 'business', 'stock', 'marketing', 'ceo', 'model', 'companies']\n\nDimension: 6\n['learning', 'neuron', 'function', 'cell', 'action', 'input', 'algorithm', 'synaptic', 'policy', 'reinforcement', 'circuit', 'control', 'weight', 'firing', 'field', 'current', 'company', 'dynamic', 'optimal', 'spike', 'direction', 'response', 'visual', 'activity', 'system']\n\nDimension: 7\n['learning', 'model', 'network', 'control', 'task', 'action', 'system', 'unit', 'reinforcement', 'controller', 'dynamic', 'learn', 'states', 'policy', 'forward', 'hidden', 'learned', 'robot', 'trajectory', 'motor', 'movement', 'human', 'step', 'trained', 'recurrent']\n\nDimension: 8\n['unit', 'input', 'training', 'object', 'image', 'hidden', 'pattern', 'learning', 'cell', 'layer', 'representation', 'recognition', 'set', 'features', 'task', 'feature', 'visual', 'images', 'output', 'information', 'map', 'direction', 'motion', 'vector', 'word']\n\nDimension: 9\n['unit', 'model', 'function', 'hidden', 'weight', 'distribution', 'input', 'layer', 'gaussian', 'parameter', 'bound', 'variables', 'approximation', 'connection', 'company', 'likelihood', 'log', 'activation', 'term', 'mean', 'density', 'energy', 'probability', 'bayesian', 'prior']\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cosine similarity analysis\ndef calculate_cosine_similarity(data_paths, dataset_name, trimmed_doc_ids=None):\n    docword_path = data_paths[dataset_name]['docword']\n    docword = pd.read_csv(docword_path, header=None, names=['docID', 'wordID', 'count'], skiprows=3, sep=' ')\n    \n    if trimmed_doc_ids:\n        docword = docword[docword['docID'].isin(trimmed_doc_ids)].reset_index(drop=True)\n    \n    wdm = docword.pivot(index='docID', columns='wordID', values='count').fillna(0).astype(pd.SparseDtype(\"int16\", 0))\n    sims = cosine_similarity(wdm, dense_output=False)\n    return sims.mean()\n\naverage_cosine_similarities = []\naverage_cosine_similarities.append((\"enron\", calculate_cosine_similarity(data_paths, 'enron', trimmed_doc_ids=set(enron['docID']))))\naverage_cosine_similarities.append((\"kos\", calculate_cosine_similarity(data_paths, 'kos')))\naverage_cosine_similarities.append((\"nips\", calculate_cosine_similarity(data_paths, 'nips')))\n\ndoc_term_mat = merged.pivot(index='docID', columns='wordID', values='count').fillna(0).astype(pd.SparseDtype(\"int16\", 0))\nsims = cosine_similarity(doc_term_mat, dense_output=False)\naverage_cosine_similarities.append((\"all\", sims.mean()))\n\ncosine_sims_df = pd.DataFrame(average_cosine_similarities, columns=['corpus', \"average_cosine_similarity\"])\ncosine_sims_df","metadata":{"execution":{"iopub.status.busy":"2024-07-08T12:01:48.803483Z","iopub.execute_input":"2024-07-08T12:01:48.804362Z","iopub.status.idle":"2024-07-08T12:03:47.271072Z","shell.execute_reply.started":"2024-07-08T12:01:48.804321Z","shell.execute_reply":"2024-07-08T12:03:47.269644Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"  corpus  average_cosine_similarity\n0  enron                  0.0256043\n1    kos                  0.0805451\n2   nips                  0.1777671\n3    all                  0.0269295","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>corpus</th>\n      <th>average_cosine_similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>enron</td>\n      <td>0.0256043</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>kos</td>\n      <td>0.0805451</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>nips</td>\n      <td>0.1777671</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>all</td>\n      <td>0.0269295</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# LSA for clustering\ndtm = wdm.T\n\nlsa = make_pipeline(TruncatedSVD(n_components=100), Normalizer(copy=False))\nx_lsa = lsa.fit_transform(dtm)\nexplained_variance = lsa[0].explained_variance_ratio_.sum()\nprint(f\"Explained variance (LSA): {explained_variance * 100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-07-08T12:01:25.822083Z","iopub.execute_input":"2024-07-08T12:01:25.822506Z","iopub.status.idle":"2024-07-08T12:01:40.853695Z","shell.execute_reply.started":"2024-07-08T12:01:25.822470Z","shell.execute_reply":"2024-07-08T12:01:40.852587Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Explained variance (LSA): 56.62%\n","output_type":"stream"}]},{"cell_type":"code","source":"#using kmeans model for the clustering \nkmeans = KMeans(n_clusters=10, max_iter=100, n_init=1)\nkmeans.fit(x_lsa)\nlabels = kmeans.labels_\n\ndtm['label'] = labels\nfor cluster in range(10):\n    print(f\"Cluster: {cluster}\")\n    cluster_docs = dtm[dtm['label'] == cluster]\n    top_ten_words = set(cluster_docs.sum().sort_values(ascending=False).head(10).index)\n    print(merged_vocab[merged_vocab['wordID'].isin(top_ten_words)]['word'].tolist())\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T12:03:49.679190Z","iopub.execute_input":"2024-07-08T12:03:49.679594Z","iopub.status.idle":"2024-07-08T12:04:45.255990Z","shell.execute_reply.started":"2024-07-08T12:03:49.679563Z","shell.execute_reply":"2024-07-08T12:04:45.254758Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Cluster: 0\n['data', 'set', 'word', 'hit', 'training', 'target', 'false', 'speech', 'alarm', 'fom']\n\nCluster: 1\n['house', 'party', 'kerry', 'dean', 'senate', 'campaign', 'democratic', 'poll', 'state']\n\nCluster: 2\n['california', 'energy', 'prices', 'company', 'market', 'power', 'cost', 'electricity', 'customer']\n\nCluster: 3\n['team', 'meeting', 'think', 'going', 'game', 'play', 'free', 'season', 'texas']\n\nCluster: 4\n['model', 'unit', 'function', 'input', 'network', 'output', 'learning', 'neural', 'system', 'neuron']\n\nCluster: 5\n['company', 'market', 'business', 'firm', 'stock', 'management', 'services', 'group', 'companies']\n\nCluster: 6\n['problem', 'model', 'data', 'set', 'function', 'network', 'training', 'error', 'learning', 'algorithm']\n\nCluster: 7\n['november', 'house', 'governor', 'kerry', 'senate', 'bush', 'poll', 'polls', 'republicans']\n\nCluster: 8\n['attached', 'contract', 'point', 'data', 'price', 'order', 'number', 'mid', 'page']\n\nCluster: 9\n['general', 'president', 'kerry', 'bush', 'campaign', 'war', 'administration', 'iraq', 'bushs']\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# PCA for clustering\ntdm = merged.pivot(index='wordID', columns='docID', values='count').fillna(0.0)\npca = PCA(n_components=100)\ny_pca = pca.fit_transform(tdm)\n\nword_pca = pd.DataFrame(y_pca, index=merged_vocab['word'])\n\nexplained_variance_pca = pca.explained_variance_ratio_.sum()\nprint(f\"Explained variance (PCA): {explained_variance_pca * 100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-07-08T12:06:02.654128Z","iopub.execute_input":"2024-07-08T12:06:02.655120Z","iopub.status.idle":"2024-07-08T12:06:44.303609Z","shell.execute_reply.started":"2024-07-08T12:06:02.655082Z","shell.execute_reply":"2024-07-08T12:06:44.302482Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Explained variance (PCA): 57.55%\n","output_type":"stream"}]},{"cell_type":"code","source":"for index in range(10):\n    thrsh = word_pca.loc[:, index].quantile(0.9)\n    print(f\"Dimension: {index}\")\n    words = list(word_pca[word_pca[index] > thrsh].sort_values([index], ascending=False).index)[:25]\n    print(words)\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T12:07:24.305103Z","iopub.execute_input":"2024-07-08T12:07:24.305388Z","iopub.status.idle":"2024-07-08T12:07:24.399054Z","shell.execute_reply.started":"2024-07-08T12:07:24.305364Z","shell.execute_reply":"2024-07-08T12:07:24.397994Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Dimension: 0\n['network', 'model', 'learning', 'input', 'function', 'neural', 'set', 'unit', 'data', 'training', 'algorithm', 'system', 'output', 'weight', 'error', 'problem', 'result', 'number', 'method', 'parameter', 'pattern', 'neuron', 'vector', 'point', 'layer']\n\nDimension: 1\n['company', 'power', 'energy', 'california', 'electricity', 'market', 'billion', 'davis', 'business', 'plan', 'companies', 'firm', 'stock', 'prices', 'price', 'plant', 'customer', 'cost', 'states', 'bill', 'financial', 'utility', 'month', 'investor', 'group']\n\nDimension: 2\n['network', 'unit', 'input', 'neural', 'output', 'weight', 'layer', 'hidden', 'net', 'training', 'company', 'pattern', 'connection', 'neuron', 'recurrent', 'architecture', 'activation', 'trained', 'firm', 'propagation', 'chip', 'delay', 'analog', 'fund', 'threshold']\n\nDimension: 3\n['model', 'neuron', 'cell', 'input', 'visual', 'system', 'response', 'signal', 'object', 'field', 'motion', 'activity', 'firing', 'direction', 'synaptic', 'stimulus', 'cortex', 'frequency', 'neural', 'circuit', 'spike', 'connection', 'cortical', 'orientation', 'spatial']\n\nDimension: 4\n['pst', 'columbia', 'mid', 'avista', 'aquila', 'epme', 'morgan', 'bpa', 'idacorpene', 'mieco', 'detm', 'emmt', 'aet', 'hafslund', 'eesi', 'engage', 'montana', 'sncl', 'psc', 'border', 'pinwest', 'chelan', 'sink', 'cinergy', 'pwx']\n\nDimension: 5\n['company', 'firm', 'fund', 'investor', 'partner', 'management', 'ventures', 'technology', 'capital', 'round', 'investment', 'product', 'venturewire', 'services', 'software', 'data', 'top', 'mail', 'financial', 'business', 'stock', 'model', 'marketing', 'ceo', 'companies']\n\nDimension: 6\n['learning', 'neuron', 'function', 'cell', 'action', 'input', 'algorithm', 'synaptic', 'policy', 'reinforcement', 'control', 'circuit', 'weight', 'firing', 'company', 'field', 'current', 'dynamic', 'optimal', 'spike', 'direction', 'response', 'visual', 'activity', 'system']\n\nDimension: 7\n['learning', 'model', 'network', 'control', 'task', 'action', 'system', 'unit', 'reinforcement', 'controller', 'dynamic', 'learn', 'states', 'policy', 'hidden', 'forward', 'learned', 'robot', 'trajectory', 'human', 'motor', 'movement', 'trained', 'step', 'architecture']\n\nDimension: 8\n['unit', 'input', 'training', 'object', 'image', 'hidden', 'pattern', 'cell', 'learning', 'layer', 'representation', 'set', 'features', 'recognition', 'feature', 'task', 'visual', 'images', 'output', 'information', 'map', 'direction', 'vector', 'motion', 'classifier']\n\nDimension: 9\n['unit', 'model', 'function', 'hidden', 'weight', 'distribution', 'input', 'layer', 'gaussian', 'parameter', 'bound', 'variables', 'approximation', 'connection', 'likelihood', 'log', 'activation', 'density', 'term', 'mean', 'company', 'energy', 'probability', 'bayesian', 'prior']\n\n","output_type":"stream"}]}]}